{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class Network(object):\n",
    "    \n",
    "    def __init__(self, sizes):\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        \n",
    "        self.biases = [np.random.randn(y, 1) \n",
    "                       for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x) \n",
    "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "    \n",
    "    def feedforward(self, a):\n",
    "        \"\"\"Return the output of the network if ``a`` is input.\"\"\"\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a)+b)\n",
    "        return a\n",
    "\n",
    "    def SGD(self, training_data, epochs, mini_batch_size, lr, test_data=None):\n",
    "        if test_data: n_test = len(test_data)\n",
    "        n = len(training_data)\n",
    "        #TODO:for epoch number of times:\n",
    "            #TODO:Shuffle the training_data\n",
    "            mini_batches = #array of mini batches from training_data\n",
    "            #TODO:for each mini batch in mini_batches\n",
    "                self.update_mini_batch(mini_batch, lr)\n",
    "            if test_data:\n",
    "                print \"Epoch {0}: {1} / {2}\".format(\n",
    "                    j, self.evaluate(test_data), n_test)\n",
    "            else:\n",
    "                print \"Epoch {0} complete\".format(j)\n",
    "\n",
    "    def update_mini_batch(self, mini_batch, lr):\n",
    "        nabla_b = #Same shape as self.biases. All zeroes initially.\n",
    "        nabla_w = #Same shape as self.weights. All zeroes initially.\n",
    "        for x, y in mini_batch:\n",
    "            #How much should bias and weights change according to the \n",
    "            #magical black box - backprop?\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            #TODO: Add delta_nabla_b to nabla_b\n",
    "            #TODO: Add delta_nabla_w to nabla_w\n",
    "        self.weights = #Reduce nabla_w from self.weights. Remember to divide nabla_w by the batch size and multiply\n",
    "                       #it by the learning rate before subracting from self.weights.\n",
    "        self.biases = #Reduce nabla_b from self.biases. Remember to divide nabla_b by the batch size and multiply\n",
    "                      #it by the learning rate before subracting from self.biases.\n",
    "\n",
    "    def evaluate(self, test_data):\n",
    "        \"\"\"Return the number of test inputs for which the neural\n",
    "        network outputs the correct result. Note that the neural\n",
    "        network's output is assumed to be the index of whichever\n",
    "        neuron in the final layer has the highest activation.\"\"\"        \n",
    "        test_results = [(np.argmax(self.feedforward(x)), y)\n",
    "                        for (x, y) in test_data]\n",
    "        return sum(int(x == y) for (x, y) in test_results)\n",
    "            \n",
    "def sigmoid(z):\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
